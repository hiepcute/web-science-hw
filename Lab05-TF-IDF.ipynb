{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QACY3c_lr4uz"
   },
   "source": [
    "# Lab05: TF-IDF.\n",
    "\n",
    "- MSSV: \n",
    "- Họ và tên:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eD8WblEMr4u1"
   },
   "source": [
    "## Yêu cầu bài tập\n",
    "\n",
    "**Cách làm bài**\n",
    "\n",
    "\n",
    "Bạn sẽ làm trực tiếp trên file notebook này; từ `TODO` cho biết những phần mà bạn cần phải làm.\n",
    "\n",
    "Bạn có thể thảo luận ý tưởng cũng như tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn*. \n",
    "\n",
    "Nếu vi phạm thì sẽ bị 0 điểm cho bài tập này.\n",
    "\n",
    "**Cách nộp bài**\n",
    "\n",
    "Trước khi nộp bài, rerun lại notebook (`Kernel` -> `Restart & Run All`).\n",
    "\n",
    "Sau đó, tạo thư mục có tên `MSSV` của bạn (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) Chép file notebook, file `tf_idf_data.txt` của các bạn (nếu file này kích thước lớn các bạn có thể chép link vào `link_data.txt`), nén thư mục `MSSV` này lại và nộp trên moodle.\n",
    "\n",
    "**Nội dung bài tập**\n",
    "\n",
    "Cài đặt một web crawler để thu thập và biểu diễn dữ liệu bằng không gian vector và trọng số TF-IDF: https://en.wikipedia.org/wiki/Web_mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txFiZE0Zr4u2"
   },
   "source": [
    "## Nội dung bài tập"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pU505-86r4u4"
   },
   "source": [
    "## 1. Không gian vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8DCPXdVrr4u4"
   },
   "source": [
    "- Một vector 2 chiều có thể được viết dưới dạng `[x,y]`\n",
    "- Một vector 3 chiều có thể được viết dưới dạng `[x,y,z]`\n",
    "![vector](vector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đặt $V$ là tập hợp các đặc trưng trong thể hiện dữ liệu.\n",
    "- Bất kỳ mẫu dữ liệu nào đều có thể được biểu diễn dưới dạng một vectơ với $\\vert V\\vert$ chiều\n",
    "- Ví dụ: giả sử chúng ta có 3 đặc trưng là các từ dog, bite, man. Giá trị 1 thể hiện từ đó xuất hiện 1 lần, 0 là không xuất hiện.  \n",
    "![space](space.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "TF-IDF cho biết độ quan trọng của một từ đối với một tài liệu trong ngữ liệu.\n",
    "\n",
    "TF- Term Frequency : dùng để ước lượng tần xuất xuất hiện của từ trong văn bản. Tuy nhiên với mỗi văn bản thì có độ dài khác nhau số lần xuất hiện của từ có thể nhiều hơn . Vì vậy số lần xuất hiện của từ sẽ được chuẩn hóa bằng cách chia cho độ dài của văn bản (tổng số từ trong văn bản đó).\n",
    "    \n",
    "$$tf_{t}=\\dfrac{f(t,d)}{\\sum_{i \\in d}f(i,d)}$$ \n",
    "\n",
    "- $f(t,d)$ là số lần xuất hiện từ $t$ trong văn bản $d$.\n",
    "\n",
    "IDF- Inverse Document Frequency: Khi tính tần số xuất hiện TF thì các từ đều được coi là quan trọng như nhau. Tuy nhiên có một số từ thường được được sử dụng nhiều nhưng không quan trọng để thể hiện ý nghĩa của đoạn văn như \"is\", \"the\"... (các từ chức năng). Ta cần giảm độ quan trọng của những từ này.\n",
    "\n",
    "$$idf_t=\\log \\left(\\dfrac{n}{df_t}\\right)$$\n",
    "\n",
    "- *n* là số văn bản.\n",
    "\n",
    "- $df_t$ là số văn bản xuất hiện từ t\n",
    "\n",
    "**TF-IDF:** $$tf_{t} \\times idf_t$$\n",
    "\n",
    "- $tf_{t}$ càng lớn tần suất xuất hiện của từ trong văn bản càng lớn.\n",
    "- $idf_t$ càng lớn từ hiếm khi xuất hiện trong tập dữ liệu.\n",
    "- **Giả định** những đặc trưng quan trọng nhất là những đặc trưng hiếm xuất hiện."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6kgzYNRKr4u5"
   },
   "source": [
    "## 3. Thu thập và thể hiện dữ liệu\n",
    "Thu thập dữ liệu bắt đầu từ URL: https://en.wikipedia.org/wiki/Web_mining.\n",
    "- Gọi $D$ là một tập văn bản chứa *n* văn bản: $D=\\left\\{d_1,d_2,...,d_n\\right\\}$.\n",
    "- $V=\\left\\{v_1,v_2,...v_{\\vert V \\vert}\\right\\}$ là từ điển (tất cả các từ phân biệt trong dữ liệu thu được). $\\vert V \\vert$ là kích thước từ điển.\n",
    "- Một trọng số $w_{ij}$ được gán cho từ $t_i$ của văn bản dj thuộc $D$ xác định mức quan trọng của $t_i$ trong văn bản $d_j$. Từ không xuất hiện trong $d_j$ có $w_{ij}=0$.\n",
    "- Mỗi văn bản $d_j$ được thể hiện dưới dạng vector $\\mathbf{d_j}= [w_{1j},w_{2j},...,w_{\\vert V \\vert j}]$\n",
    "- Thể hiện dữ liệu bằng một ma trận M kích thước $n \\times \\vert V \\vert$ => mỗi hàng thể hiện một văn bản."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "import numpy as np\n",
    "#n_urls=200\n",
    "#M=np.zeros(shape=(n_url,n_words),dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len 2\n",
      "len 3\n",
      "len 4\n",
      "len 5\n",
      "len 6\n",
      "len 7\n",
      "len 8\n",
      "len 9\n",
      "Error Connecting: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Http Error: 404 Client Error: Not Found for url: https://docs.google.com/open?id=1nU1vrz-gBtSJk3bkb1ls_QuGX2nUPPemECPFlCx0C75MvmQdSqPci6LZWJYf\n",
      "len 10\n",
      "len 11\n",
      "len 12\n",
      "len 13\n",
      "len 14\n",
      "Error Connecting: HTTPConnectionPool(host='soave.isti.cnr.it', port=80): Max retries exceeded with url: /~silvestr/wp-content/uploads/2007/03/p63-baraglia.pdf (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x0000026861BABEF0>, 'Connection to soave.isti.cnr.it timed out. (connect timeout=3)'))\n",
      "len 15\n",
      "len 16\n",
      "len 17\n",
      "len 18\n",
      "len 19\n",
      "len 20\n",
      "len 21\n",
      "len 22\n",
      "len 23\n",
      "len 24\n",
      "len 25\n",
      "Http Error: 403 Client Error: Forbidden for url: http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/wi/&amp;toc=comp/proceedings/wi/2005/2415/00/2415toc.xml&amp;DOI=10.1109/WI.2005.153\n",
      "Http Error: 404 Client Error: Page not found for url: https://www.researchgate.net/publication/230852676_Web_User_Navigation_Patterns_Discovery_from_WWW_Server_Log_Files/file/9fcfd5055ba8960606.pdf\n",
      "len 26\n",
      "Http Error: 404 Client Error:  for url: https://doi.org/10.1007/3-540-48298-9_32%3C/a%3E.%3C/li%3E%3C/ul%3E\n",
      "len 27\n",
      "len 28\n",
      "len 29\n",
      "len 30\n",
      "len 31\n",
      "len 32\n",
      "len 33\n",
      "len 34\n",
      "len 35\n",
      "len 36\n",
      "len 37\n",
      "len 38\n",
      "len 39\n",
      "len 40\n",
      "len 41\n",
      "len 42\n",
      "len 43\n",
      "len 44\n",
      "len 45\n",
      "len 46\n",
      "len 47\n",
      "len 48\n",
      "len 49\n",
      "len 50\n",
      "https://en.wikipedia.org/wiki/Web_mining\n",
      "https://en.wikipedia.org/w/index.php?title=Web_mining&amp;action=edit\n",
      "https://en.wikipedia.org/w/index.php?title=Web_mining&amp;action=edit&amp;section=\n",
      "https://archive.org/details/webinformationsy00ngua\n",
      "https://archive.org/details/webinformationsy00ngua/page/n33\n",
      "https://archive.org/details/electroniccommer00bauk_698\n",
      "https://archive.org/details/electroniccommer00bauk_698/page/n176\n",
      "https://archive.org/details/webminingapplica0000scim/page/282\n",
      "http://alexandria.tue.nl/repository/freearticles/612259.pdf\n",
      "https://ui.adsabs.harvard.edu/abs/2000cs.......11033K\n",
      "https://doi.org/10.1145%2F360402.360406\n",
      "https://api.semanticscholar.org/CorpusID:60455\n",
      "http://library.ifla.org/148/\n",
      "https://doi.org/10.5195%2Fjmla.2019.650\n",
      "http://facweb.cs.depaul.edu/mobasher/classes/ect584/papers/cms-kais.pdf\n",
      "https://link.springer.com/chapter/10.1007/978-3-540-88309-8_34\n",
      "http://robotics.stanford.edu/\n",
      "http://informationr.net/ir/11-2/paper249.html\n",
      "https://www.researchgate.net/profile/Magdalini_Eirinaki/publication/220169917_Web_mining_for_web_personalization/links/0912f51379b8daa82a000000.pdf\n",
      "http://dmr.cs.umn.edu/Papers/P2000_10.pdf\n",
      "http://facweb.cs.depaul.edu/mobasher/research/papers/widm01.pdf\n",
      "http://webmining.spd.louisville.edu/wp-content/uploads/2014/05/Nasraoui_WebKDD03_web_recomm.pdf\n",
      "http://webmining.spd.louisville.edu/wp-content/uploads/2014/05/Nasraoui-IFSA-99-mining-web-access-logs.pdf\n",
      "http://webmining.spd.louisville.edu/wp-content/uploads/2014/05/FINAL-Nasraoui-WWW-Personalization.pdf\n",
      "http://www.chris-kimble.com/Publications/Documents/Ting_2005b.pdf\n",
      "https://doi.org/10.1007/3-540-48298-9_32\n",
      "https://en.wikipedia.org/w/index.php?title=Web_mining&amp;oldid=1021193642\n",
      "https://en.wikipedia.org/w/index.php?title=Web_mining&amp;oldid=1021193642</a>\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q785337\n",
      "https://ar.wikipedia.org/wiki/%D8%AA%D9%86%D9%82%D9%8A%D8%A8_%D8%A7%D9%84%D9%88%D9%8A%D8%A8\n",
      "https://de.wikipedia.org/wiki/Web_Mining\n",
      "https://es.wikipedia.org/wiki/Web_mining\n",
      "https://eu.wikipedia.org/wiki/Web-meatzaritza\n",
      "https://fa.wikipedia.org/wiki/%D9%88%D8%A8%E2%80%8C%DA%A9%D8%A7%D9%88%DB%8C\n",
      "https://fr.wikipedia.org/wiki/Fouille_du_web\n",
      "https://ko.wikipedia.org/wiki/%EC%9B%B9_%EB%A7%88%EC%9D%B4%EB%8B%9D\n",
      "https://hi.wikipedia.org/wiki/%E0%A4%85%E0%A4%82%E0%A4%A4%E0%A4%B0%E0%A4%9C%E0%A4%BE%E0%A4%B2_%E0%A4%96%E0%A4%A8%E0%A4%A8\n",
      "https://hr.wikipedia.org/wiki/Dubinsko_pretra%C5%BEivanje_interneta\n",
      "https://hu.wikipedia.org/wiki/Webb%C3%A1ny%C3%A1szat\n",
      "https://ja.wikipedia.org/wiki/%E3%82%A6%E3%82%A7%E3%83%96%E3%83%9E%E3%82%A4%E3%83%8B%E3%83%B3%E3%82%B0\n",
      "https://pt.wikipedia.org/wiki/Minera%C3%A7%C3%A3o_da_web\n",
      "https://ru.wikipedia.org/wiki/Web_mining\n",
      "https://sk.wikipedia.org/wiki/Web_mining\n",
      "https://th.wikipedia.org/wiki/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%97%E0%B8%B3%E0%B9%80%E0%B8%AB%E0%B8%A1%E0%B8%B7%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%A7%E0%B9%87%E0%B8%9A\n",
      "https://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\n",
      "https://stats.wikimedia.org/\n",
      "https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "https://wikimediafoundation.org/\n",
      " broken_url\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import string\n",
    "import pickle\n",
    "def get_urls(url):\n",
    "    r = requests.get(url)\n",
    "    # TODO\n",
    "    # Lấy các url nằm trong trang web của url này, lưu lại vào biến urls\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', r.text)\n",
    "    urls_new=[]\n",
    "    for i in urls:\n",
    "        if (i not in urls_new):\n",
    "            urls_new.append(i)\n",
    "    return urls_new\n",
    "broken_url=[]\n",
    "def get_urls_recursive(start_url, limit):\n",
    "    urls = [start_url]\n",
    "    for url in urls:\n",
    "        # TODO\n",
    "        # Lấy các url nằm trong trang web của url này, lưu lại vào biến new_urls\n",
    "        # Với mỗi url mới trong new_urls:\n",
    "        #   Nếu nó chưa nằm trong urls thì thêm nó vô  \n",
    "        # Nếu kích thước của urls vượt quá limit thì dừng và xóa phần dư thừa\n",
    "        new=get_urls(url)# lây  tất cả các url của trang web lưu vào biến new\n",
    "        news=[]\n",
    "        # ta sẽ kiểm tra hết các url trong new nếu url không còn hoạt động nữa thì ta sẽ remove nó đi\n",
    "            \n",
    "        for i in new: # kiểm tra các url trong new có nằm trong list urls hay ko\n",
    "            if i not in urls:\n",
    "                if len(urls)>49:\n",
    "                    break\n",
    "                else:\n",
    "                    try: \n",
    "                        response = requests.get(i,timeout=3) \n",
    "                        response.raise_for_status()    \n",
    "                        urls.append(i)           # Raise error in case of failure \n",
    "                        print(\"len\",len(urls))\n",
    "                        if(len(urls)>49):\n",
    "                            break\n",
    "                    #print(\"urls\",urls)\n",
    "                    except requests.exceptions.HTTPError as httpErr: \n",
    "                        print (\"Http Error:\",httpErr) \n",
    "                    except requests.exceptions.ConnectionError as connErr: \n",
    "                        print (\"Error Connecting:\",connErr) \n",
    "                    except requests.exceptions.Timeout as timeOutErr: \n",
    "                        print (\"Timeout Error:\",timeOutErr) \n",
    "                    except requests.exceptions.RequestException as reqErr: \n",
    "                        print (\"Something Else:\",reqErr) \n",
    "        if(len(urls)>49):\n",
    "            break\n",
    "                  \n",
    "\n",
    "    return urls\n",
    "url_list = get_urls_recursive('https://en.wikipedia.org/wiki/Web_mining',50)\n",
    "for i in url_list:\n",
    "    print(i)\n",
    "print(\" broken_url\")\n",
    "for i in broken_url:\n",
    "    print(i)\n",
    "print(len(url_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_filter(element):\n",
    "    if element.parent.name in ['style', 'title', 'script', 'head', '[document]', 'class', 'a', 'li']:\n",
    "        return False\n",
    "    elif isinstance(element, Comment):\n",
    "        '''Opinion mining?'''\n",
    "        return False\n",
    "    elif re.match(r\"[\\s\\r\\n]+\",str(element)): \n",
    "        '''space, return, endline'''\n",
    "        return False\n",
    "    return True\n",
    "def wordList(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    text = soup.findAll(text=True)\n",
    "    filtered_text = list(filter(text_filter, text)) # list của các chuỗi\n",
    "    word_list = []\n",
    "    \n",
    "    for i in range(0,len(filtered_text)):\n",
    "        if filtered_text[i] in string.punctuation:# kiểm tra xem có trong list kí tự hay không\n",
    "            filtered_text[i]= filtered_text[i].replace(filtered_text[i],\" \")#replace \" \" # thay thế bằng khoảng trắng\n",
    "    for i in range(0,len(filtered_text)):\n",
    "        list_w_split=[]\n",
    "        list_w_split=filtered_text[i].split(\" \")# tách các chuỗi thành các câu lưu vào word_list\n",
    "        for j in range(0,len(list_w_split)):\n",
    "            word_list.append(list_w_split[j])\n",
    "    for i in word_list:\n",
    "        if i in string.punctuation:# check word_list thêm một lần nữa, nếu có khoảng trắng thì remove\n",
    "            word_list.remove(i)\n",
    "    return word_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Web' 'mining' 'From' ... '' '' '.\\t\\t']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# tạo từ điển  bằng cách đọc hết các url , thêm vào mảng data\n",
    "data=[]\n",
    "for i in url_list:\n",
    "    word_list=wordList(i)\n",
    "    data=np.concatenate((data, word_list), axis=None)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130508"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "print(english_stopwords)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122261\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Loại bỏ các key của biến data mà nằm trong danh sách english_stopwords\n",
    "data = list(data)\n",
    "for i in data:\n",
    "    if i in english_stopwords:# kiểm tra xem key có trong english top w ko\n",
    "        data.remove(i)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004020100502512563\n"
     ]
    }
   ],
   "source": [
    "# ham tinh trong so của 1 từ \n",
    "def weight_word(url,word):\n",
    "    #count số lần xuất hiện của từ đó \n",
    "    list_word=wordList(url)\n",
    "    count = list_word.count(word)\n",
    "    return count/len(list_word)\n",
    "#example\n",
    "count=weight_word('https://en.wikipedia.org/wiki/Web_mining',\"web\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "import numpy as np\n",
    "n_url=len(url_list)\n",
    "M=np.zeros(shape=(n_url,len(data)),dtype=float)# nurl 100, len data 122261\n",
    "length=len(data)\n",
    "for i in range(n_url):\n",
    "    list_word=wordList(url_list[i])\n",
    "    for j in range(length):\n",
    "        if data[j] in list_word:\n",
    "            M[i][j]=list_word.count(data[j])/len(list_word)\n",
    "        else:\n",
    "            M[i][j]=0\n",
    "\n",
    "            \n",
    "np.savetxt(\"tf_idf_data.txt\", M, fmt=\"%4d\", delimiter=\",\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab03-WebCrawler.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
